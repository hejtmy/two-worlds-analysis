---
title: "Transfer report"
author: "Lukáš Hejtmánek"
date: "9 August 2018"
output: github_document
---

```{r setup, include=FALSE}
#needs lme and car in the top because of the recode
library(car)
library(lme4)
library(ggplot2)
library(ez)
library(Hmisc)
library(dplyr)
library(reshape2)
library(broom)
library(knitr)
library(papaja)

source("../Scripts/paper-prepare.R")
#green, violet, blue, yellow, pink, dark blue
estimote_pal <- c('#b6d6c1', '#54003d','#93d5f6','#d5d215','#f3aacb','#2d2556')
theme_set(theme_minimal())

condition_all_sums <- sub_walk_all %>% 
  group_by(exp_block_id, id, learning.condition) %>% 
  summarise(sum.distance = sum(min_norm_distance, na.rm = T), 
            sum.errors = sum(errors, na.rm = T),
            mean.distance = mean(min_norm_distance, na.rm = T),
            mean.error = mean(errors, na.rm = T))

block_measure <- function(df, value){
  df_wide <- dcast(df, learning.condition + id ~ exp_block_id, value.var = value)
  colnames(df_wide) <- c("learning.condition","id","block1", "block2","block3","block4","block5","block6")
  df_wide %>% 
    group_by(learning.condition) %>%
    summarise(block1.mean  = mean(block1, na.rm = T),
              block3.mean = mean(block3, na.rm = T),
              block4.mean = mean(block4, na.rm = T),
              block34.diff = mean((block3-block4)/(block4+block3), na.rm = T),
              block34.se  = sqrt(var((block3-block4)/(block4+block3), na.rm = T)/sum(!is.na(block3-block4))),
              block14.diff = mean((block1-block4)/(block1+block4), na.rm = T),
              block14.se = sqrt(var((block1-block4)/(block1+block4), na.rm = T)/sum(!is.na(block1-block4)))
  )
}
df_errors <- block_measure(condition_all_sums, "sum.errors")
df_distance <- block_measure(condition_all_sums, "mean.distance")

df_wide_errors <- dcast(condition_all_sums, learning.condition + id ~ exp_block_id, value.var = "sum.errors")
df_wide_distance <- dcast(condition_all_sums, learning.condition + id ~ exp_block_id, value.var = "mean.distance")
colnames(df_wide_errors) <- c("learning.condition","id","block1", "block2","block3","block4","block5","block6")
colnames(df_wide_distance) <- c("learning.condition","id","block1", "block2","block3","block4","block5","block6")
```
 
## Hypotheses
1. There will be some transfer in all learning conditions.
2. Transfer will be higher in the real world learning condition than in desktop condition.
3. Transfer will be higher in the real world learning condition than in VR treadmill condition.
4. Transfer will be higher in the VR treadmill condition that in the desktop condition.

- transfer is operationalised as a change in performance in navigational tasks following the environment/modality switch.

Specific tests

- Overrall average walking performance in phase 1 vs phase 2 
- Performance in block 4 vs performance in block 1 and 2 
- Overrall average pointing performance in phase 1 vs phase 2 
- Average walking performance in block 3 vs 4 (block directly before vs directly after the switch)
- Personal walking performance change in block 3 vs 4 (calculated as an average of individuals (block3-block4)/(block3+block4))
- Personal walking performance change in block 1 vs 4 (calculated as an average of individuals (block1-block4)/(block3+block4))
- Learning modality has an effect on the learning curve (slope)
- Level of correlation of personal performance in block 1,2,3 and 4 - higher transfer should show higher correlation in these comparisons
- Difference in performance between groups in blocks 5 and 6 (e.g. not immediately after the switch, but after all groups have been subjected the real world environment for some time)
- Difference in performance between groups in block 4-6 with inclusion of VR-VR group nad real-VR group (to see if the final performance is subjected to the necessity of a real world experience, or not and to see if the swith has detrimental effect on real-VR as well).

## Controlling analyses
1. Every condition starts on a same level of performance (there isn't a bias in the first block)

## Procedure description
I ran all tests on the normalised distance, errors are not normalised, as they are the same in all conditions. 

I only selected those "important" comparisons - therefore Desktop to Real world, Treadmill VR to real world and Real world to real world. Anovas, mixed models and other comparisons are run only on these. Learning conditions (block 1-3) are the different modalities, and blocks 4-6 are all real world testing.


## Descriptives
In normalised distance we can see that people in different conditions start fairly at the same level of performance (difference between learning conditions is not significant `r apa_print(aov(min_norm_distance ~ learning.condition, sub_walk_all[sub_walk_all$exp_block_id == 1, ]))$statistic$learning_condition`). Nor there is significant difference between all 5 starting groups `r apa_print(aov(min_norm_distance ~ condition, walk_all[walk_all$exp_block_id == 1, ]))$statistic$condition`.

```{r, echo=F, warning=F, message=F}
make_graph(sub_walk_all, "learning.condition", "min_norm_distance", "exp_block_id") + 
  ylab("Average normalised trial distance") + xlab("condition") + 
  scale_fill_manual(values=estimote_pal,"Block ID")

ggplot(sub_walk_all, aes(x = exp_block_id, y = min_norm_distance, color = factor(learning.condition))) +
  stat_summary(fun.data=mean_cl_normal,position=position_dodge(width=0.1), width = 0.2, geom="errorbar") + 
  stat_summary(fun.y=mean,position=position_identity(), size = 1, geom="line") + 
  geom_vline(xintercept = 3.5, linetype=4) +
  ylab("Average normalised trial distance") + xlab("Block") + 
  scale_color_manual(values=estimote_pal,"Condition") + guides(fill=F)
```

There is difference in normalised times in all conditions in the first block (`r apa_print(aov(min_norm_time ~ learning.condition, sub_walk_all[sub_walk_all$exp_block_id == 1, ]))$statistic$learning_condition`)). Probably due to learning issues with the treadmill. Threre is no significant difference between desktop and real world learning in normalised times (`r apa_print(t.test(min_norm_time ~ learning.condition, sub_walk_all[sub_walk_all$exp_block_id == 1 & sub_walk_all$learning.condition %in% c("Desktop", "Real"), ]))$statistic`))

```{r, echo=F}
make_graph(walk_all, "condition", "min_norm_time", "exp_block_id")+ 
  ylab("Average normalised trial duration") + xlab("condition") + 
  scale_fill_manual(values=estimote_pal, "Block ID")
```

And we don't have any significant difference in number of errors in the first block (`r apa_print(aov(errors ~ learning.condition, sub_walk_all[sub_walk_all$exp_block_id == 1, ]))$statistic$learning_condition`)) nor for the all 5 condition (`r apa_print(aov(errors ~ condition, walk_all[walk_all$exp_block_id == 1, ]))$statistic$condition`).

```{r, echo=F}
make_graph(walk_all, "condition", "errors", "exp_block_id")+ 
  ylab("Average number of errors") + xlab("Condition") + 
  scale_fill_manual(values=estimote_pal," Block ID")

ggplot(sub_walk_all, aes(x = exp_block_id, y = errors, color = factor(learning.condition))) +
  stat_summary(fun.data=mean_cl_normal,position=position_dodge(width=0.1), width = 0.2, geom="errorbar") + 
  stat_summary(fun.y=mean,position=position_identity(), size = 1, geom="line") + 
  geom_vline(xintercept = 3.5, linetype=4) +
  ylab("Number of mistakes in office decision") + xlab("Block number") + 
  scale_color_manual(values=estimote_pal, "Condition")
```


## Specific predictions testing

### Performance in block 4 is better than in block 1

If we assume some level of transfer, than the average or individual performance in block 4 should be better in all conditions than in block 1 or potentially in block 2. I don't test for block 3 because that immediately compares the switch performance. 
```{r, echo=F, message=F, warning=F}
make_graph(sub_walk_all[sub_walk_all$exp_block_id %in% c(1,4),], "learning.condition", "min_norm_distance", "exp_block_id") + 
  ylab("Average normalised trial distance") + xlab("condition") + 
  scale_fill_manual(values=estimote_pal,"Block ID")
make_graph(sub_walk_all[sub_walk_all$exp_block_id %in% c(1,4),], "learning.condition", "errors", "exp_block_id") + 
  ylab("Average number of errors") + xlab("condition") + 
  scale_fill_manual(values=estimote_pal,"Block ID")
```

```{r, echo=F}
df_dist <- block_t_test(sub_walk_all, 1, 4,"min_norm_distance")[,c(1,4)]
df_error <- block_t_test(sub_walk_all, 1, 4,"errors")[,c(1,4)]
df14 <- merge(df_dist, df_error, by = "learning.condition")
colnames(df14) <- c("learning condition", "p-value distance improvement", "p-value errors improvement")
kable(df14, caption="Paired T test comparing infividual performance improvement from block 1-4 in different conditions")
```

We see that both real world and treadmill VR learning leads to improvement from block 1 to block 4 in both errors and path travelled. But desktop only in number of errors and marginally in distance improvement. All this suggests transfer is happening across all learning conditions.

### Overrall average walking and error performance in phase 1 vs phase 2
Given that all participants had 3 blocks of trials in the second phase done in the real world, We can assume, if one learning method leads to better transfer than another, than the performance improvement between phases could be different as well.
```{r, echo=F, warning=F, message=F}
make_graph(sub_walk_all, "learning.condition", "min_norm_distance", "phase") + 
  ylab("Average normalised trial distance") + xlab("condition") + 
  scale_fill_manual(values=estimote_pal,"Phase")
make_graph(sub_walk_all, "learning.condition", "errors", "phase")+ 
  ylab("Average number of errors") + xlab("condition") + 
  scale_fill_manual(values=estimote_pal,"Phase")
```

Mixed models show a significant effect of phase on overall performance, but not of their interaction.
```{r, echo=F}
fit_dist_phase <- lmer(min_norm_distance ~ phase*learning.condition + (1 | id/phase), data = sub_walk_all)
kable(mixed_table_report(fit_dist_phase))
pair_t_norm_distance_phase_2 <- pairwise.t.test(sub_walk_all$min_norm_distance, sub_walk_all$learning.condition, p.adj = "holm")
```

Performance in the second phase differs significantly between groups (`r apa_print(aov(min_norm_distance ~ learning.condition, sub_walk_all[sub_walk_all$phase == 2,]))$statistic$learning_condition`), suggesting that the learnign modality has an effect on the overall performance. Tukey test shows significant difference between real adn a desktop versions. although this can be only because of bad performance in block 4 after switch, not due to blocks 5 and 6 (will test later.)

```{r, echo=F}
df_temp <- TukeyHSD(x=aov(min_norm_distance ~ learning.condition, sub_walk_all[sub_walk_all$phase == 2,]), conf.level=0.95)$learning.condition[, c(1,4)]
kable(tukey_report(df_temp))
```

### Overrall average pointing performance in phase 1 vs phase 2
The issue with comparing pointing is that people point after learning condition and then only after being subjected to the new condition for 18 trials. Which means that each participant has experienced the real world for half an hour before pointing. As we see laterr, the biggest difference in transfer seems to occur between blocks 3-4, with most participants achieving same performance by the block 6. That means that the comparison of pointing is not very informative.

```{r, echo=F, warning=F, message=F}
make_graph(sub_sop_all, "learning.condition", "abs_error", "phase") + 
  ylab("Average normalised trial distance") + xlab("condition") + 
  scale_fill_manual(values=estimote_pal,"Phase")
```

Mixed models show effect of both phase, learning condition and their interaction on pointing performance. Probably due to the fact that participants pointed less acurately on the treadmill and evn less on the desktop, but all then achieved same performance by the end of the 6th block.

```{r, echo=F}
fit_angle_phase <- lmer(abs_error ~ phase*learning.condition + (1 | id/phase), data = sub_sop_all)
kable(mixed_table_report(fit_angle_phase))
```


### Comparing walking performance in block 3 vs 4

Looking directly at the performance after the switch, we can see if there is a sudden drop in path length desktop condition. We can study this using paired t.tests as well as non paired t-tests. Looking at the graphs we see differences in the level to which people have learned the environemtn in the 3rd block.  

```{r, echo=F, warning=F, message=F}
make_graph(sub_walk_all[sub_walk_all$exp_block_id %in% c(3,4),], "learning.condition", "min_norm_distance", "exp_block_id") + 
  ylab("Average normalised trial distance") + xlab("condition") + 
  scale_fill_manual(values=estimote_pal,"Block ID")
make_graph(sub_walk_all[sub_walk_all$exp_block_id %in% c(3,4),], "learning.condition", "errors", "exp_block_id") + 
  ylab("Average number of errors") + xlab("condition") + 
  scale_fill_manual(values=estimote_pal,"Block ID")
```

Anova doesn't show shows a significant difference between groups in the block 3 (`r apa_print(aov(min_norm_distance ~ learning.condition, sub_walk_all[sub_walk_all$exp_block_id == 3,]))$statistic$learning_condition`), but there is a significant difference between distance performance in the block 4 (`r apa_print(aov(min_norm_distance ~ learning.condition, sub_walk_all[sub_walk_all$exp_block_id == 4,]))$statistic$learning_condition`).

There is significant difference in the number of errors in both block 3 (`r apa_print(aov(errors ~ learning.condition, sub_walk_all[sub_walk_all$exp_block_id == 3,]))$statistic$learning_condition`), and the block 4 (`r apa_print(aov(errors ~ learning.condition, sub_walk_all[sub_walk_all$exp_block_id == 4,]))$statistic$learning_condition`)., with treadmill and desktop performing worse than real world.

Mixed model shows significant effect of both block and its interaction on distance walked.
```{r, echo=F}
fit_dist_34 <- lmer(min_norm_distance ~ exp_block_id*learning.condition + (1 | id/exp_block_id), data = sub_walk_all[sub_walk_all$exp_block_id %in% c(3,4),])
kable(mixed_table_report(fit_dist_34))
```

And significant effect all both conditoin, block and their interaction on the number of errors.
```{r, echo=F}
fit_dist_34 <- lmer(errors ~ exp_block_id*learning.condition + (1 | id/exp_block_id), data = sub_walk_all[sub_walk_all$exp_block_id %in% c(3,4),])
kable(mixed_table_report(fit_dist_34))
```

Running separate pairwise t-tests to see individual performance change from block 3 to 4 gives the following results.
```{r, echo=F}
df_dist <- block_t_test(sub_walk_all, 3, 4,"min_norm_distance")[,c(1,4)]
df_error <- block_t_test(sub_walk_all, 3, 4,"errors")[,c(1,4)]
df34 <- merge(df_dist, df_error, by = "learning.condition")
colnames(df34) <- c("learning condition", "p-value distance improvement", "p-value errors improvement")
kable(df34, caption="Paired T test comparing infividual performance improvement from block 3-4 in different conditions")
```

WE can see that all participants improved in the errors made, but neigher group is significantly worse between block 3 and 4 in the path travelled (although participants learning on desktop perform slightly worse in 4th block, but the significance is only marginal)


### Personal walking performance change in block 3 vs 4 

```{r, echo=F}
df_wide_errors$block34 <- (df_wide_errors$block3-df_wide_errors$block4)/(df_wide_errors$block3+df_wide_errors$block4)
df_wide_distance$block34 <- (df_wide_distance$block3-df_wide_distance$block4)/(df_wide_distance$block3+df_wide_distance$block4)
clean_walk_34 <- ez_prepare_block(sub_walk_all, c(3,4))
```
The other approach is using the "performance change" coefficient as you proposed calculated as an average of individual's (block3-block4)/(block3+block4))

Comparing the block perfomance change in different conditions using anovas, we see marginally significant difference in the improvement between groups for distance (`r apa_print(aov(block34 ~ learning.condition, df_wide_distance))$statistic$learning_condition`).

```{r, echo=F}
ggplot(df_distance, aes(x = learning.condition, y = block34.diff, fill = learning.condition)) + 
  geom_bar(stat = 'identity', position="dodge") + 
  geom_errorbar(aes(ymin=block34.diff-block34.se, ymax=block34.diff+block34.se), width=.2, position=position_dodge(.9)) +
  scale_fill_manual(values = estimote_pal) +
  ylab("Mean normalised distance improvement from block 3 to 4") + xlab("Learning condition")  + guides(fill=F)
```

```{r, warning = F, echo=F, message=F}
ggplot(sub_walk_all[sub_walk_all$exp_block_id %in% c(3,4),], aes(x = as.numeric(exp_block_id), y = min_norm_distance, color = factor(learning.condition))) +
  stat_summary(fun.data=mean_cl_normal,position=position_dodge(width=0.1), width = 0.2, geom="errorbar") + 
  stat_summary(fun.y=mean,position=position_identity(), size = 1, geom="line") + 
  ylab("Average normalised distance") + xlab("Experimental block") + 
  scale_color_manual(values = estimote_pal, "Learning condition")
```

We can see significant differences between different conditions in error rate improvement from block 3 to 4 when running independent anova (`r apa_print(aov(block34 ~ learning.condition, df_wide_errors))$statistic$learning_condition`).

Post hoc tests show significant difference between Real and Desktop and marginally for Treadmill and Desktop.
```{r, echo=F}
kable(tukey_report(TukeyHSD(x=aov(block34 ~ learning.condition, df_wide_errors), conf.level=0.95)$learning.condition[, c(1,4)]))
```

```{r, echo=F}
ggplot(df_errors, aes(x = learning.condition, y = block34.diff, fill = learning.condition)) + 
  geom_bar(stat = 'identity', position="dodge") + 
  geom_errorbar(aes(ymin=block34.diff-block34.se, ymax=block34.diff+block34.se), width=.2, position=position_dodge(.9)) +
  scale_fill_manual(values = estimote_pal) +
  ylab("Mean sum of errors improvement after switch")+ xlab("Learning condition") + guides(fill=F)
```

```{r, warning = F, echo=F, message=F}
ggplot(sub_walk_all[sub_walk_all$exp_block_id %in% c(3,4),], aes(x = as.numeric(exp_block_id), y = errors, color = factor(condition))) +
  stat_summary(fun.data=mean_cl_normal,position=position_dodge(width=0.1), width = 0.2, geom="errorbar") + 
  stat_summary(fun.y=mean,position=position_identity(), size = 1, geom="line") + 
  ylab("Average number of errors") + xlab("Experimental block") + 
  scale_color_manual(values = estimote_pal, "Learning condition")
```

### Personal walking performance change in block 1 vs 4 
(calculated as an average of individuals (block1-block4)/(block3+block4))

```{r, echo=F}
df_wide_errors$block14 <- (df_wide_errors$block1-df_wide_errors$block4)/(df_wide_errors$block1+df_wide_errors$block4)
df_wide_distance$block14 <- (df_wide_distance$block1-df_wide_distance$block4)/(df_wide_distance$block1+df_wide_distance$block4)
clean_walk_14 <- ez_prepare_block(sub_walk_all, c(1,4))
```

Anova shows no significant difference in the improvement between groups for distance (`r apa_print(aov(block14 ~ learning.condition, df_wide_distance))$statistic$learning_condition`). This might be due to quite large variability in the sample. 

```{r, echo=F}
ggplot(df_distance, aes(x = learning.condition, y = block14.diff, fill = learning.condition)) + 
  geom_bar(stat = 'identity', position="dodge") + 
  geom_errorbar(aes(ymin=block14.diff-block14.se, ymax=block14.diff+block14.se), width=.2, position=position_dodge(.9)) +
  scale_fill_manual(values = estimote_pal) +
  ylab("Mean normalised distance improvement from block 1 to 4") + xlab("Learning condition")  + guides(fill=F)
```

But it reveals significant difference between learning conditions in number of errors improvement from block 1 to block 4 (`r apa_print(aov(block14 ~ learning.condition, df_wide_errors))$statistic$learning_condition`). 

Runnign Tukey's post hoc on the distance shows that this is originating from the difference between Desktop and the Real world, with Treadmill not being significantly different from either of them.

```{r, echo=F}
kable(tukey_report(TukeyHSD(x=aov(block14 ~ learning.condition, df_wide_errors), conf.level=0.95)$learning.condition[, c(1,4)]))
```

```{r, echo=F}
ggplot(df_errors, aes(x = learning.condition, y = block14.diff, fill = learning.condition)) + 
  geom_bar(stat = 'identity', position="dodge") + 
  geom_errorbar(aes(ymin=block14.diff-block14.se, ymax=block14.diff+block14.se), width=.2, position=position_dodge(.9)) +
  scale_fill_manual(values = estimote_pal) +
  ylab("Mean sum of errors improvement from block 1 to 4") + xlab("Learning condition")  + guides(fill=F)
```


### Learning modality has an effect on the learning curve (slope)
I hope mike or professor Ferer will help with this

### Level of correlation of personal performance in block 1,2,3 and 4

This is a bit weird way of comparing things and it should be somewhat duplicit to the mixed models, but we can measure how much performance from 3-3 correlates between different conditions. I don't know if it isn't actually measured in the mixed models, Mike should know this better.

Table for 
```{r, echo=F, warning=F}
df_dist<-block_correlation(sub_walk_all, 3,4, "min_norm_distance")
df_err<-block_correlation(sub_walk_all, 3,4, "errors")
df_out <- merge(df_dist, df_err, by = "learning.condition")
colnames(df_out) <- c("Learning condition", "Correlation of normalised distance", "Correlation of number of errors")
kable(df_out)
```

I don't know how to interpret this properly, but it looks as the data before suggested - that the least correlation is between desktop in 3rd block and real in 4th. whereas in real to real, the performance correlates very well. The number of errors seems to improve in all cases, so that is why we see medium correlation is across the board.

### Difference in performance between groups in blocks 5 and 6 
Not sure if this is interesting or we should focus mainly on the slope analyses.

### Difference in performance between groups in block 4-6 with inclusion of VR-VR group and real-VR group 

And here I am not sure if the inclusion of the other groups affects the results in some way - especially in mixed models, Mike said we should pick a model and stick with it. I have an intuition in how to use the other groups to "confirm" or elaborate on our results, but I am unsure how to formulate it or how specifically to run the analyses.

## Summary
We see effect of learning condition (blocks 1-3) on performance in block 4 by t-tests in all conditions separately.

We see positive improvement in distance from block 1 to 4 in all conditions and this improvement is not significantly different across learning groups. We see similar improvement in number of errors, but Desktop improves significantly worse than real world training.

We see improvement from phase 1-2 in both walking, errors and pointing precision.

Participant's performance in distance from block 3-4 gets slightly worse after the switch distance-wise (but not significanly).

number of errors decreases (improves) across all  conditions., but desktop learning shows least amount of improvement from block 3-4 and 1-4, participants doing significantly worse than those with real world training and marginally worse than those with Treadmill training. It may mean that they know which door are correct, but they are unsure in how to get to them, showing some level of memory transfer, but not necessarily spatial knowledge transfer.

Only desktop shows worse distance performance in block 4 (marginal significance),. treadmil-real and real-real stays relatively the same (good transfer - not improvement).

There is a significant effect in mixed models of learning condition and interaction on distance (marginal effect of learning condition) and errors from block 3-4.

Personal distance performance in blocks 3 and 4 in real world and treadmil doesn't correlate much, but real to real does. Correlation of error performance is similar across all conditions.

We see somewhat clearly the trend Real >= Treadmill VR > Desktop, but often only marginally, due to large variability of the result. Treadmil and real are often different in an expected way but not significantly different. We usually get significant differences only in real to desktop comparisons.

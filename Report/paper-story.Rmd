---
title: "Transfer report"
author: "Lukáš Hejtmánek"
date: "9 August 2018"
output: html_document
---

```{r setup, include=FALSE}
library(ggplot2)
library(ez)
library(Hmisc)
library(dplyr)
library(reshape2)
library(broom)
library(knitr)
library(papaja)

source("../Scripts/paper-prepare.R")
#green, violet, blue, yellow, pink, dark blue
estimote_pal <- c('#b6d6c1', '#54003d','#93d5f6','#d5d215','#f3aacb','#2d2556')
theme_set(theme_minimal())

condition_all_sums <- sub_walk_all %>% 
  group_by(exp_block_id, id, learning.condition) %>% 
  summarise(sum.distance = sum(min_norm_distance, na.rm = T), 
            sum.errors = sum(errors, na.rm = T),
            mean.distance = mean(min_norm_distance, na.rm = T),
            mean.error = mean(errors, na.rm = T))

block_measure <- function(df, value){
  df_wide <- dcast(df, learning.condition + id ~ exp_block_id, value.var = value)
  colnames(df_wide) <- c("learning.condition","id","block1", "block2","block3","block4","block5","block6")
  df_wide %>% 
    group_by(learning.condition) %>%
    summarise(block1.mean  = mean(block1, na.rm = T),
              block3.mean = mean(block3, na.rm = T),
              block4.mean = mean(block4, na.rm = T),
              block34.diff = mean((block3-block4)/(block4+block3), na.rm = T),
              block34.se  = sqrt(var((block3-block4)/(block4+block3), na.rm = T)/sum(!is.na(block3-block4))),
              block14.diff = mean((block1-block4)/(block1+block4), na.rm = T),
              block14.se = sqrt(var((block1-block4)/(block1+block4), na.rm = T)/sum(!is.na(block1-block4)))
  )
}
```
 There will be smaller spatial knowledge transfer in those conditions which use less 
 
## Hypotheses
1. There will be some transfer in all learning conditions.
2. Transfer will be higher in the real world learning condition than in desktop condition.
3. Transfer will be higher in the real world learning condition than in VR treadmill condition.
4. Transfer will be higher in the VR treadmill condition that in the desktop condition.

- transfer is operationalised as a change in performance in navigational tasks following the environment/modality switch.

Specific tests (hypothesis in parenthesis)
- Overrall average walking performance in phase 1 vs phase 2 
- Performance in block 4 vs performance in block 1 and 2 

- Overrall average pointing performance in phase 1 vs phase 2 
- Average walking perfomrance in block 3 vs 4 (block directly before vs directly after the switch)
- Personal walking performance change in block 3 vs 4 (calculated as an average of individuals (block3-block4)/(block3+block4))
- Personal walking performance change in block 1 vs 4 (calculated as an average of individuals (block1-block4)/(block3+block4))
- level of correlation of personal performance in block 1,2,3 and 4 - higher transfer should show higher correlation in these comparisons
- Difference in performance between groups in blocks 5 and 6 (e.g. not immediately after the switch, but after all groups have been subjected the real world environment for some time)
- Difference in performance between groups in block 4-6 with inclusion of VR-VR group nad real-VR group (to see if the final performance is subjected to the necessity of a real world experience, or not and to see if the swith has detrimental effect on real-VR as well).

## Controlling analyses
1. Every condition starts on a same level of performance (there isn't a bias in the first block)

## Procedure description
I ran all tests on the normalised distance, errors are not normalised, as they are the same in all conditions. 

I only selected those "important" comparisons - therefore Desktop to Real world, Treadmill VR to real world and Real world to real world. Anovas and other comparisons are run only on these. Learning conditions (block 1-3) are the different modalities, and blocks 4-6 are all real world testing.

## Descriptives
In normalised distance we can see that people in different conditions start fairly at the same level of performance (difference between learnign conditions is not significant `r apa_print(aov(min_norm_distance ~ learning.condition, sub_walk_all[sub_walk_all$exp_block_id == 1, ]))$statistic$learning_condition`). Nor there is significant difference between all 5 starting groups `r apa_print(aov(min_norm_distance ~ learning.condition, sub_walk_all[sub_walk_all$exp_block_id == 1, ]))$statistic$condition`.

```{r, echo=F, warning=F}
make_graph(sub_walk_all, "learning.condition", "min_norm_distance", "exp_block_id") + 
  ylab("Average normalised trial distance") + xlab("condition") + 
  scale_fill_manual(values=estimote_pal,"Block ID")

ggplot(sub_walk_all, aes(x = exp_block_id, y = min_norm_distance, color = factor(learning.condition))) +
  stat_summary(fun.data=mean_cl_normal,position=position_dodge(width=0.1), width = 0.2, geom="errorbar") + 
  stat_summary(fun.y=mean,position=position_identity(), size = 1, geom="line") + 
  geom_vline(xintercept = 3.5, linetype=4) +
  ylab("Average normalised trial distance") + xlab("Block") + 
  scale_color_manual(values=estimote_pal,"Condition") + guides(fill=F)
```

There is difference in normalised times in all conditions in the first block (`r apa_print(aov(min_norm_time ~ learning.condition, sub_walk_all[sub_walk_all$exp_block_id == 1, ]))$statistic$learning_condition`)). Probably due to learning issues with the treadmill. Threre is no significant difference between desktop and real world learning in normalised times (`r apa_print(t.test(min_norm_time ~ learning.condition, sub_walk_all[sub_walk_all$exp_block_id == 1 & sub_walk_all$learning.condition %in% c("Desktop", "Real"), ]))$statistic`))

```{r, echo=F}
make_graph(walk_all, "condition", "min_norm_time", "exp_block_id")+ 
  ylab("Average normalised trial duration") + xlab("condition") + 
  scale_fill_manual(values=estimote_pal, "Block ID")
```

And we don't have any significant difference in number of errors in the first block (`r apa_print(aov(errors ~ learning.condition, sub_walk_all[sub_walk_all$exp_block_id == 1, ]))$statistic$learning_condition`)) nor for the all 5 condition (`r apa_print(aov(errors ~ condition, walk_all[walk_all$exp_block_id == 1, ]))$statistic$condition`).

```{r, echo=F}
make_graph(walk_all, "condition", "errors", "exp_block_id")+ 
  ylab("Average number of errors") + xlab("Condition") + 
  scale_fill_manual(values=estimote_pal," Block ID")

ggplot(sub_walk_all, aes(x = exp_block_id, y = errors, color = factor(learning.condition))) +
  stat_summary(fun.data=mean_cl_normal,position=position_dodge(width=0.1), width = 0.2, geom="errorbar") + 
  stat_summary(fun.y=mean,position=position_identity(), size = 1, geom="line") + 
  geom_vline(xintercept = 3.5, linetype=4) +
  ylab("Number of mistakes in office decision") + xlab("Block number") + 
  scale_color_manual(values=estimote_pal, "Condition")
```

## Specific predictions testing

### Performance in block 4 is better than in block 1
If we assume some level of transfer, than the average or individual performance in block 4 should be better in all conditions than in block 1 or potentially in block 2. I don't test for block 3 because that immediately compares the switch performance. 
```{r, echo=F,message=F}
make_graph(sub_walk_all[sub_walk_all$exp_block_id %in% c(1,4),], "learning.condition", "min_norm_distance", "exp_block_id") + 
  ylab("Average normalised trial distance") + xlab("condition") + 
  scale_fill_manual(values=estimote_pal,"Block ID")
make_graph(sub_walk_all[sub_walk_all$exp_block_id %in% c(1,4),], "learning.condition", "errors", "exp_block_id") + 
  ylab("Average number of errors") + xlab("condition") + 
  scale_fill_manual(values=estimote_pal,"Block ID")
```

```{r, echo=F}
df_dist <- block_t_test(sub_walk_all, 1, 4,"min_norm_distance")[,c(1,4)]
df_error <- block_t_test(sub_walk_all, 1, 4,"errors")[,c(1,4)]
df14 <- merge(df_dist, df_error, by = "learning.condition")
colnames(df14) <- c("learning condition", "p-value distance improvement", "p-value errors improvement")
kable(df14, caption="Paired T test comparing infividual performance improvement from block 1-4 in different conditions")
```

We see that both real world and treadmill VR learning leads to improvement from block 1 to block 4 in both errors and path travelled. But desktop only in number of errors and marginally in distance improvement.

### Overrall average walking and error performance in phase 1 vs phase 2
Given that all participants had 3 blocks of trials in the second phase done in the real world, We can assume, if one learning method leads to better transfer than another, than the performance improvement between phases could be different.
```{r, echo=F, warning=F}
make_graph(sub_walk_all, "learning.condition", "min_norm_distance", "phase") + 
  ylab("Average normalised trial distance") + xlab("condition") + 
  scale_fill_manual(values=estimote_pal,"Phase")
make_graph(sub_walk_all, "learning.condition", "errors", "phase")+ 
  ylab("Average number of errors") + xlab("condition") + 
  scale_fill_manual(values=estimote_pal,"Phase")
```

Anova shows a significant effect of phase and condition on overall performance, but not of their interaction.
```{r, echo=F}
apa_print(aov(min_norm_distance ~ phase*learning.condition, sub_walk_all))
pair_t_norm_distance_phase_2 <- pairwise.t.test(sub_walk_all$min_norm_distance, sub_walk_all$learning.condition, p.adj = "holm")
```
Looking solely at the performance differences in the second phase differs significantly between groups (`r apa_print(aov(min_norm_distance ~ learning.condition, sub_walk_all[sub_walk_all$phase == 2,]))$statistic$learning_condition`). Tukey test shows significant difference between real adn a desktop versions

```{r, echo=F}
kable(TukeyHSD(x=aov(min_norm_distance ~ learning.condition, sub_walk_all[sub_walk_all$phase == 2,]), conf.level=0.95)$learning.condition[, c(1,4)])
```

### Overrall average pointing performance in phase 1 vs phase 2
```{r, echo=F, warning=F}
make_graph(sub_sop_all, "learning.condition", "abs_error", "phase") + 
  ylab("Average normalised trial distance") + xlab("condition") + 
  scale_fill_manual(values=estimote_pal,"Phase")
```

Anova shows significant effect of both phase, learning condition and their interaction on pointing performance.
```{r, echo=F}
kable(apa_print(aov(abs_error ~ phase*learning.condition, sub_sop_all))$table)
```

### Average walking perfomrance in block 3 vs 4

Looking directly at the performance after the switch, we can see if there is a sudden drop in performance. We can study this both suing individual paired t.tests as well as non paired t-tests. Looking at the graphs we see differences in the level to which people have learned the environemtn in the 3rd block.  
```{r, echo=F, message=F}
make_graph(sub_walk_all[sub_walk_all$exp_block_id %in% c(3,4),], "learning.condition", "min_norm_distance", "exp_block_id") + 
  ylab("Average normalised trial distance") + xlab("condition") + 
  scale_fill_manual(values=estimote_pal,"Block ID")
make_graph(sub_walk_all[sub_walk_all$exp_block_id %in% c(3,4),], "learning.condition", "errors", "exp_block_id") + 
  ylab("Average number of errors") + xlab("condition") + 
  scale_fill_manual(values=estimote_pal,"Block ID")
```

Anova doesn¨t show shows a significant difference in the block 3 (`r apa_print(aov(min_norm_distance ~ learning.condition, sub_walk_all[sub_walk_all$exp_block_id == 3,]))$statistic$learning_condition`), but there is a significant difference between path performance in the block 4 (`r apa_print(aov(min_norm_distance ~ learning.condition, sub_walk_all[sub_walk_all$exp_block_id == 4,]))$statistic$learning_condition`).

There is significant difference in the number of errors in both block 3 (`r apa_print(aov(errors ~ learning.condition, sub_walk_all[sub_walk_all$exp_block_id == 3,]))$statistic$learning_condition`), and the block 4 (`r apa_print(aov(errors ~ learning.condition, sub_walk_all[sub_walk_all$exp_block_id == 4,]))$statistic$learning_condition`).

Running pairwise t-tests to see performance change from block 3 to 4 gives the following results.
```{r, echo=F}
df_dist <- block_t_test(sub_walk_all, 3, 4,"min_norm_distance")[,c(1,4)]
df_error <- block_t_test(sub_walk_all, 3, 4,"errors")[,c(1,4)]
df34 <- merge(df_dist, df_error, by = "learning.condition")
colnames(df34) <- c("learning condition", "p-value distance improvement", "p-value errors improvement")
kable(df34, caption="Paired T test comparing infividual performance improvement from block 3-4 in different conditions")
```
WE can see that all participants improved in the errors made, but neigher group is significantly worse between block 3 and 4 in the path travelled (although participants learning on desktop perform slightly worse in 4th block, but the significance is only marginal)

### Personal walking performance change in block 3 vs 4 
(calculated as an average of individuals (block3-block4)/(block3+block4))
```{r, echo=F}

```

### Personal walking performance change in block 1 vs 4 
(calculated as an average of individuals (block1-block4)/(block3+block4))
```{r, echo=F}

```

### level of correlation of personal performance in block 1,2,3 and 4

- higher transfer should show higher correlation in these comparisons
```{r, echo=F}

```

POtential followups
### difference in performance between groups in blocks 5 and 6 

(e.g. not immediately after the switch, but after all groups have been subjected the real world environment for some time)
```{r, echo=F}

```
### difference in performance between groups in block 4-6 with inclusion of VR-VR group nad real-VR group 

(to see if the final performance is subjected to the necessity of a real world experience, or not and to see if the swith has detrimental effect on real-VR as well).
```{r, echo=F}

```
But I can look at either pure changes between block 3-4 or 1-4 or those calculations that you proposed as a (block4-block3)/(block3+block4) measurements.

```{r, echo=F}
df_wide_errors <- dcast(condition_all_sums, learning.condition + id ~ exp_block_id, value.var = "sum.errors")
df_wide_distance <- dcast(condition_all_sums, learning.condition + id ~ exp_block_id, value.var = "mean.distance")
colnames(df_wide_errors) <- c("learning.condition","id","block1", "block2","block3","block4","block5","block6")
colnames(df_wide_distance) <- c("learning.condition","id","block1", "block2","block3","block4","block5","block6")

df_wide_errors$block14 <- (df_wide_errors$block1-df_wide_errors$block4)/(df_wide_errors$block1+df_wide_errors$block4)
df_wide_errors$block34 <- (df_wide_errors$block3-df_wide_errors$block4)/(df_wide_errors$block3+df_wide_errors$block4)
df_wide_distance$block14 <- (df_wide_distance$block1-df_wide_distance$block4)/(df_wide_distance$block1+df_wide_distance$block4)
df_wide_distance$block34 <- (df_wide_distance$block3-df_wide_distance$block4)/(df_wide_distance$block3+df_wide_distance$block4)

df_errors <- block_measure(condition_all_sums, "sum.errors")
df_distance <- block_measure(condition_all_sums, "mean.distance")

clean_walk_14 <- ez_prepare_block(sub_walk_all, c(1,4))
clean_walk_34 <- ez_prepare_block(sub_walk_all, c(3,4))
```
## Block improvement from block 1 to block 4

But anova shows no significant difference in the improvement between groups for distance (`r apa_print(aov(block14 ~ learning.condition, df_wide_distance))$statistic$learning_condition`). This might be due to quite large variability in the sample.

```{r, echo=F}
ggplot(df_distance, aes(x = learning.condition, y = block14.diff, fill = learning.condition)) + 
  geom_bar(stat = 'identity', position="dodge") + 
  geom_errorbar(aes(ymin=block14.diff-block14.se, ymax=block14.diff+block14.se), width=.2, position=position_dodge(.9)) +
  scale_fill_manual(values = estimote_pal) +
  ylab("Mean normalised distance improvement from block 1 to 4") + xlab("Learning condition")  + guides(fill=F)
```

### Errors
Unlike for distance, anova reveals significant difference between learning conditions in number of errors improvement from block 1 to block 4 (`r apa_print(aov(block14 ~ learning.condition, df_wide_errors))$statistic$learning_condition`). 

Runnign Tukey's post hoc on the distance shows that this is originating from the difference between Desktop and the Real world, with Treadmill not being significantly different from either of them.

```{r, echo=F}
kable(TukeyHSD(x=aov(block14 ~ learning.condition, df_wide_errors), conf.level=0.95)$learning.condition[, c(1,4)])
```

```{r, echo=F}
ggplot(df_errors, aes(x = learning.condition, y = block14.diff, fill = learning.condition)) + 
  geom_bar(stat = 'identity', position="dodge") + 
  geom_errorbar(aes(ymin=block14.diff-block14.se, ymax=block14.diff+block14.se), width=.2, position=position_dodge(.9)) +
  scale_fill_manual(values = estimote_pal) +
  ylab("Mean sum of errors improvement from block 1 to 4") + xlab("Learning condition")  + guides(fill=F)
```

## Block performance improvement from block 3 to block 4

Everybody's distance performance (including Real to Real condition) gets a little bit worse from 3 to 4. But this is not significant when computed using paired t-tests, although it is marginally worse in the desktop version. But all conditions show decrease in the number of errors from block 3 to 4.

```{r, echo=F}
df_dist <- block_t_test(sub_walk_all, 3, 4,"min_norm_distance")[,c(1,4)]
df_error <- block_t_test(sub_walk_all, 3, 4,"errors")[,c(1,4)]
df34 <- merge(df_dist, df_error, by = "learning.condition")
colnames(df34) <- c("learning.condition", "p-value distance improvement", "p-value errors improvement")
kable(df34)
```

### Distance

Comparing the block improvement in different conditions using anovas, we see marginally significant difference in the improvement between groups for distance (`r apa_print(aov(block34 ~ learning.condition, df_wide_distance))$statistic$learning_condition`).

```{r, echo=F}
ggplot(df_distance, aes(x = learning.condition, y = block34.diff, fill = learning.condition)) + 
  geom_bar(stat = 'identity', position="dodge") + 
  geom_errorbar(aes(ymin=block34.diff-block34.se, ymax=block34.diff+block34.se), width=.2, position=position_dodge(.9)) +
  scale_fill_manual(values = estimote_pal) +
  ylab("Mean normalised distance improvement from block 3 to 4") + xlab("Learning condition")  + guides(fill=F)
```

Within subject anova for distance improvement between 3 and 4 shows significant interaction effect and marginally significant effect of learning condition and block on distance.
```{r, warning = F, echo=F, message=F}
aov_block_dist_34 <- ezANOVA(
  data = clean_walk_34,
  dv = min_norm_distance, 
  wid = id,
  within = exp_block_id,
  between = learning.condition, 
  type = 3)
kable(aov_block_dist_34)

ggplot(clean_walk_34, aes(x = as.numeric(exp_block_id), y = min_norm_distance, color = factor(learning.condition))) +
  stat_summary(fun.data=mean_cl_normal,position=position_dodge(width=0.1), width = 0.2, geom="errorbar") + 
  stat_summary(fun.y=mean,position=position_identity(), size = 1, geom="line") + 
  ylab("Average normalised distance") + xlab("Experimental block") + 
  scale_color_manual(values = estimote_pal, "Learning condition")
```

### Errors

We can see significant differences between different conditions in error rate improvement from block 3 to 4 when running independent anova (`r apa_print(aov(block34 ~ learning.condition, df_wide_errors))$statistic$learning_condition`).

Post hoc tests show significant difference between Real and Desktop and marginally for Treadmill and Desktop.
```{r, echo=F}
kable(TukeyHSD(x=aov(block34 ~ learning.condition, df_wide_errors), conf.level=0.95)$learning.condition[, c(1,4)])
```

```{r, echo=F}
ggplot(df_errors, aes(x = learning.condition, y = block34.diff, fill = learning.condition)) + 
  geom_bar(stat = 'identity', position="dodge") + 
  geom_errorbar(aes(ymin=block34.diff-block34.se, ymax=block34.diff+block34.se), width=.2, position=position_dodge(.9)) +
  scale_fill_manual(values = estimote_pal) +
  ylab("Mean sum of errors improvement after switch")+ xlab("Learning condition") + guides(fill=F)
```

Running within subjects anova for block 3-4 number of errors shows significant effect of both condition and block, but unlike in distance improvement, their interaction doesn't seem to have an effect on error rate improvement.
```{r, warning = F, echo=F, message=F}
aov_block_err_34 <- ezANOVA(
  data = clean_walk_34,
  dv = errors, 
  wid = id,
  within = exp_block_id,
  between = condition,
  type = 3)
kable(aov_block_err_34)

ggplot(clean_walk_34, aes(x = as.numeric(exp_block_id), y = errors, color = factor(condition))) +
  stat_summary(fun.data=mean_cl_normal,position=position_dodge(width=0.1), width = 0.2, geom="errorbar") + 
  stat_summary(fun.y=mean,position=position_identity(), size = 1, geom="line") + 
  ylab("Average number of errors") + xlab("Experimental block") + 
  scale_color_manual(values = estimote_pal, "Learning condition")
```

## Summary
We see effect of learning condition (blocks 1-3) on performance in block 4.

We see positive improvement in distance from block 1 to 4 in all conditions and this improvement is not significantly different across learning groups. We see similar improvement in number of errors, but Desktop improves significantly worse than real world training.

Participant's performance in distance from block 3-4 gets slightly worse after the switch distance-wise (but not significanly), but their number of errors decrease across all conditions. It can mean that they know which door are correct, but they are unsure in how to get to them, showing some level of memory transfer, but not necessarily spatial knowledge transfer.

We can see marginally significant difference in block 3-4 distance improvement acrosss learning conditions, with desktop doing the worst but not significantly different from the rest. Within subject anova for distance improvement in blocks 3-4 shows interaction between learning condition and experimental block - only desktop gets "worse" after the switch of environment. 

But we can see clear error rate improvement differences across conditions - all participants improved, but desktop learning shows least amount of improvement from block 3-4, participants doing significantly worse than those with real world training and marginally worse than those with Treadmill training.



---
title: "R Notebook"
output: github_document
---

```{r setup, include=FALSE}
#needs lme and car in the top because of the recode
library(car)
library(lme4)
library(lmerTest)
library(ggplot2)
library(ez)
library(Hmisc)
library(dplyr)
library(reshape2)
library(broom)
library(knitr)
library(papaja)
library(googlesheets)

source("../Scripts/paper-prepare.R")
source("helpers/block-t-test-improvement.R")
source("helpers/block-t-test.R")
source("helpers/block-measure.R")

settings <- load_google_sheets()
df_participants <- settings$participants
df_participants <- df_participants %>% mutate(condition=paste0(First.phase,"-",Second.phase))
df_participants <- df_participants %>% filter(condition %in% c("vr-real", "real-real", "ve-real"))
overview <- settings$versions
overview <- overview %>% mutate(condition=paste0(First.phase,"-",Second.phase))
overview <- overview %>% filter(condition %in% c("vr-real", "real-real", "ve-real"))

#green, violet, blue, yellow, pink, dark blue
estimote_pal <- c('#b6d6c1', '#54003d','#93d5f6','#d5d215','#f3aacb','#2d2556')
theme_set(theme_minimal())

### Try running it without outliers
sub_walk_all[!is.na(sub_walk_all$norm_distance) & sub_walk_all$norm_distance > 3, c("distance", "min_norm_distance")] <- NA

condition_all_sums <- sub_walk_all %>% 
  group_by(exp_block_id, id, learning.condition) %>% 
  summarise(sum.distance = sum(min_norm_distance, na.rm = T), 
            sum.errors = sum(errors, na.rm = T),
            mean.distance = mean(min_norm_distance, na.rm = T),
            mean.error = mean(errors, na.rm = T))


df_errors <- block_measure(condition_all_sums, "sum.errors")
df_distance <- block_measure(condition_all_sums, "mean.distance")

df_wide_errors <- dcast(condition_all_sums, learning.condition + id ~ exp_block_id, value.var = "sum.errors")
df_wide_distance <- dcast(condition_all_sums, learning.condition + id ~ exp_block_id, value.var = "mean.distance")
colnames(df_wide_errors) <- c("learning.condition","id","block1", "block2","block3","block4","block5","block6")
colnames(df_wide_distance) <- c("learning.condition","id","block1", "block2","block3","block4","block5","block6")
df_wide_errors$block34 <- (df_wide_errors$block3-df_wide_errors$block4)/(df_wide_errors$block3+df_wide_errors$block4)
df_wide_distance$block34 <- (df_wide_distance$block3-df_wide_distance$block4)/(df_wide_distance$block3+df_wide_distance$block4)

knitr::opts_chunk$set(echo=F, warning = F, message = F)

```

# Methods

## Measures
We calculated participantsâ€™ walked distance in the building, time spent in each task and number of incorrectly visited doors. The conditions were not perfectly comparable in their measured distance and time, with the VR trials taking a little bit longer in both time and distance than the real world condition. We therefore min-normalised the distance traveled and the time for each task and environment  by dividing the measure by any participant's performance for that trial. We calculated all statistics on these normalised measures. 

There was no difference between conditions at the start of the experiment in normalised distance (`r apa_print(aov(min_norm_distance ~ learning.condition, sub_walk_all[sub_walk_all$exp_block_id == 1, ]))$statistic$learning_condition`), nor in number errors (`r apa_print(aov(errors ~ learning.condition, sub_walk_all[sub_walk_all$exp_block_id == 1, ]))$statistic$learning_condition`), but there is a significant difference in normalised time (`r apa_print(aov(min_norm_time ~ learning.condition, sub_walk_all[sub_walk_all$exp_block_id == 1, ]))$statistic$learning_condition`)), due to VR taking longer times to get used to (and therefore having longer normalised trial times at the start). We removed distance measurements larger than 3 standard deviations from the mean.

## Participants
```{r}
finished <- overview %>% group_by(condition) %>% count(finished)
finished <- dcast(finished, condition~finished)
finished$percent <- round(finished$yes/(finished$yes+finished$no), 2)
is_ok <- overview %>% filter(finished=="yes") %>% group_by(condition) %>% count(is_ok)
is_ok <- dcast(is_ok, condition~is_ok)
is_ok$percent <- round(is_ok$yes/(is_ok$yes+is_ok$no), 2)
```
A total of `r sum(overview$is_ok == "yes", na.rm=T)` (M = `r round(mean(df_participants[df_participants$finished == "yes",]$age, na.rm=T),1)`, SD = `r round(sd(df_participants[df_participants$finished == "yes",]$age, na.rm=T), 1)`) undergraduate students at UC Davis particiapted in the study in exchange for a study credit. Each students was randomly assigned a condition and a randomised set of goals. `r sum(finished$no)` participants didn't finish due to motion sickness and `r sum(is_ok$no)` were removed due to a technical failure of the real world tracking systen. Only `r finished[finished$condition == "vr-real","percent"] * 100` percent the students being able to finish VR to Real world condition.

# Results

First we wanted to assess that there is going to be some level of transfer in all conditions. If we would assume no transfer to happen, we should observe same performance in the real world testing in 1st, 2nd and 3rd block after the switch to be similar in performance as the 1st, 2nd and 3rd block in the real world learning group. But all conditions show better performance after the switch (see table XXX) than in the first trial, suggesting some level of transfer happening in all learning conditions.
```{r}
df <- data.frame(condition=rep("",3),dist=rep("",3), err=rep("",3), stringsAsFactors = F)
df_dist <- block_t_test_improvement(sub_walk_all, 1, 4,"min_norm_distance")
df_error <- block_t_test_improvement(sub_walk_all, 1, 4,"errors")
for(i in 1:length(names(df_dist))){
  name <- names(df_dist)[i]
  df[i,"condition"] <- name
  df[i, "dist"] <- apa_print(df_dist[[name]])$full_result
  df[i, "err"] <- apa_print(df_error[[name]])$full_result
}
colnames(df) <- c("Learning condition", "Distance improvement", "Errors improvement")
df$`Learning condition` <- dplyr::recode(df$`Learning condition`, "real-real" = "Real", "ve-real" = "Desktop", "vr-real" = "Treadmill VR", "real-vr" = "Real", "vr-vr" = "Treadmill VR")
kable(df, caption="Paired T test comparing individual performance improvement from block 1-4 in different conditions")
```

Looking at the progression in distance improvements and error improvements over the course of the experiment, we can see that all conditions show progressive improvement which is only hindered by the switch in the desktop learning condition (see fig. XXX and fig. XXX), but in the Treadmill condition we see constant improvement unobstructed by the environment switch.
```{r}
ggplot(sub_walk_all, aes(x = exp_block_id, y = min_norm_distance, color = factor(learning.condition))) +
  stat_summary(fun.data=mean_cl_normal,position=position_dodge(width=0.1), width = 0.5, geom="errorbar") + 
  stat_summary(fun.y=mean,position=position_identity(), size = 1.25, geom="line") + 
  geom_vline(xintercept = 3.5, linetype=4) +
  ylab("Average normalised trial distance") + xlab("Block") + 
  scale_color_manual(values=estimote_pal,"Learning condition") + guides(fill=F)

ggplot(sub_walk_all, aes(x = exp_block_id, y = errors, color = factor(learning.condition))) +
  stat_summary(fun.data=mean_cl_normal,position=position_dodge(width=0.1), width = 0.5, geom="errorbar") + 
  stat_summary(fun.y=mean,position=position_identity(), size = 1.25, geom="line") + 
  geom_vline(xintercept = 3.5, linetype=4) +
  ylab("Number of mistakes in office decision") + xlab("Block number") + 
  scale_color_manual(values=estimote_pal, "Learning condition")
```

Participant's performance in the first pre-switch phase shows different rates of learning rate, demonstrated by an interaction between block bumber and learning condition for error rate performance (see table XX). 

**this is a mixed model with id/block id random term. Significance was measuyred with Anova from car package, but I read it is not a good way to measure significance and we shouldn't even use significance in mixed models. I'll check on past cognition papers to see how and if they do it.**

```{r}
learning_slopes_distance <- lmer(min_norm_distance ~ learning.condition*exp_block_id + (1 | id/exp_block_id), data = sub_walk_all[sub_walk_all$exp_block_id %in% 1:3,])
tab_distance <- mixed_table_report(learning_slopes_distance)

learning_slopes_errors <- lmer(errors ~ learning.condition*exp_block_id + (1 | id/exp_block_id), data = sub_walk_all[sub_walk_all$exp_block_id %in% 1:3,])
tab_errors <- mixed_table_report(learning_slopes_errors)

tab <- merge(tab_distance[,c(3,4)], tab_errors[,c(3,4)], by = "names")
tab$names <- c("Intercept", "Block number", "Learning condition", "Block - Learning condition interaction")
colnames(tab) <- c("Predictor", "Significance for distance improvement", "Significance for error improvement")
kable(tab)
#ggplot(sub_walk_all[sub_walk_all$exp_block_id %in% 1:3,], aes(exp_block_id, min_norm_distance, color=learning.condition)) + geom_point() + geom_smooth(method="lm", se=F)
#ggplot(sub_walk_all[sub_walk_all$exp_block_id %in% 1:3,], aes(exp_block_id, errors, color=learning.condition)) + geom_point() + geom_smooth(method="lm", se=F)

```


### After modality switch performance change
We found significant difference between different modalities  in the pre-switch block in distance performance `r apa_print(aov(min_norm_distance ~ learning.condition, only_blocks(sub_walk_all, 3)))$statistic`, and the error rate as well `r  apa_print(aov(errors ~ learning.condition, only_blocks(sub_walk_all, 3)))$statistic`. 

**Running the tukey on these???**
```{r}
aov_point_phase1 <- aov(abs_error ~ learning.condition, data = sub_sop_all[sub_sop_all$phase == 1,])
```
There was also a difference between conditions in their pointing performance at the end of the first phase (`r apa_print(aov_point_phase1)$statistic$learning_condition`), with real group performing significantly better than the Treadmill VR group `r tukey_report(TukeyHSD(aov_point_phase1), condition = "Treadmill VR-Real", delta_name = "error")` and the Desktop group `r tukey_report(TukeyHSD(aov_point_phase1), condition = "Real-Desktop", delta_name = "error")`.

After the modality switch, we can see a significant difference between groups in distance performance (`r apa_print(aov(min_norm_distance ~ learning.condition, sub_walk_all[sub_walk_all$exp_block_id == 4,]))$statistic$learning_condition`) and errors as well (`r apa_print(aov(errors ~ learning.condition, sub_walk_all[sub_walk_all$exp_block_id == 4,]))$statistic$learning_condition`), with groups learning on the treadmill or desktop performing worse than those learning in the real world. This between group difference is still present in the 2nd block of the second phase (block 5) in error rate `r  apa_print(aov(errors ~ learning.condition, only_blocks(sub_walk_all, 5)))$statistic` and marginally in distance `r apa_print(aov(min_norm_distance ~ learning.condition, only_blocks(sub_walk_all, 5)))$statistic`. In the last testing block (block 6), we found no differences among the groups in either error rate `r apa_print(aov(errors ~ learning.condition, only_blocks(sub_walk_all, 6)))$statistic`, nor pointing performance `r apa_print(aov(abs_error ~ learning.condition, data = sub_sop_all[sub_sop_all$phase == 2,]))$statistic$learning_condition`) but we still see small difference in distance performance `r apa_print(aov(min_norm_distance ~ learning.condition, only_blocks(sub_walk_all, 6)))$statistic`.

Mixed effect models with individual random effect show significant effect of the of the learning condition on rate of improvement from pre-switch to post-switch block (See table XXX).

```{r}
learning_slopes_distance <- lmer(min_norm_distance ~ learning.condition*exp_block_id + (1 | id/exp_block_id), data = only_blocks(sub_walk_all, 3:4))
tab_distance <- mixed_table_report(learning_slopes_distance)

learning_slopes_errors <- lmer(errors ~ learning.condition*exp_block_id + (1 | id/exp_block_id), data = only_blocks(sub_walk_all, 3:4))
tab_errors <- mixed_table_report(learning_slopes_errors)

tab <- merge(tab_distance[,c(3,4)], tab_errors[,c(3,4)], by = "names")
tab$names <- c("Intercept", "Block number", "Learning condition", "Block - Learning condition interaction")
colnames(tab) <- c("Predictor", "Significance for distance improvement", "Significance for error improvement")
kable(tab)
```

Running separate pairwise t-tests to see individual performance change from pre-switch to post-switch block, we see significant improvement in errors made across all conditions, and marginally significant worsening in distance performance in the desktop condition.
```{r, echo=F}
df <- data.frame(condition=rep("",3),dist=rep("",3), err=rep("",3), stringsAsFactors = F)
df_dist <- block_t_test_improvement(sub_walk_all, 3, 4,"min_norm_distance")
df_error <- block_t_test_improvement(sub_walk_all, 3, 4,"errors")
for(i in 1:length(names(df_dist))){
  name <- names(df_dist)[i]
  df[i,"condition"] <- name
  df[i, "dist"] <- apa_print(df_dist[[name]])$full_result
  df[i, "err"] <- apa_print(df_error[[name]])$full_result
}
colnames(df) <- c("Learning condition", "Distance improvement", "Errors improvement")
df$`Learning condition` <- dplyr::recode(df$`Learning condition`, "real-real" = "Real", "ve-real" = "Desktop", "vr-real" = "Treadmill VR", "real-vr" = "Real", "vr-vr" = "Treadmill VR")
kable(df, caption="Paired T test comparing individual performance improvement from pre switch block to post-switch block in different conditions")
```

We can see that all participants improved in the errors made, but neigher group is significantly worse in the post-switch block in the path travelled (although participants who learned on the desktop performed slightly worse, the significance is only marginal).

### Performance change 
To assess the level of performance change form directly before and after the switch, we calculated a personal improvement score as (block3-block4)/(block3+block4)Â´. This allowed us to directly comapre the percentual improvement or deterioration from pre-switch block to the post-switch one.

```{r, echo=F}
ggplot(df_distance, aes(x = learning.condition, y = block34.diff, fill = learning.condition)) + 
  geom_bar(stat = 'identity', position="dodge") + 
  geom_errorbar(aes(ymin=block34.diff-block34.se, ymax=block34.diff+block34.se), width=.2, position=position_dodge(.9)) +
  scale_fill_manual(values = estimote_pal) +
  ylab("Mean normalised distance improvement from block 3 to 4") + xlab("Learning condition")  + guides(fill=F)

ggplot(df_errors, aes(x = learning.condition, y = block34.diff, fill = learning.condition)) + 
  geom_bar(stat = 'identity', position="dodge") + 
  geom_errorbar(aes(ymin=block34.diff-block34.se, ymax=block34.diff+block34.se), width=.2, position=position_dodge(.9)) +
  scale_fill_manual(values = estimote_pal) +
  ylab("Mean sum of errors improvement after switch")+ xlab("Learning condition") + guides(fill=F)
```

Comparing the perfomance change in different conditions using ANOVA, we see marginally significant difference between groups in the distance improvement (`r apa_print(aov(block34 ~ learning.condition, df_wide_distance))$statistic$learning_condition`), but significant differences in error rate improvement (`r apa_print(aov(block34 ~ learning.condition, df_wide_errors))$statistic$learning_condition`). This is consistent with the mixed model result which poited at interaction between block and learning condition.

Tukey post-hoc tests show significant difference between error rate improvement between the group that learned in the real world and that which learned on the desktop.
```{r, echo=F}
kable(tukey_report_table(TukeyHSD(x=aov(block34 ~ learning.condition, df_wide_errors), conf.level=0.95), "learning.condition"))
```


### Improvement from the pre-switch block to the final block
Assuming the performance platoes around the 6the block, looking at participants performance in the pre-switch block and the last block we can deduce what margin of improvement is still possible. We see that participants learning in the real world no longer improve in error rate after the 3rd block `r apa_print(block_t_test_improvement(sub_walk_all, 4, 6, "errors")[["real-real"]])$statistic`, but they are continuously improving in distance performance `r apa_print(block_t_test_improvement(sub_walk_all, 4, 6, "min_norm_distance")[["real-real"]])$statistic`. 

*Not sure about the mixed models here. I think they are OK, but I haven't published anything with them before*

Looking at the linear mixed model prediction for the distnace and error improvement in the second phase (post-switch), we can see that both learning condition and 

```{r}
learning_slopes_distance <- lmer(min_norm_distance ~ learning.condition*exp_block_id + (1 | id/exp_block_id), data = only_blocks(sub_walk_all[sub_walk_all$min_norm_distance < 10, ], 4:6))
tab_distance <- mixed_table_report(learning_slopes_distance)

learning_slopes_errors <- lmer(errors ~ learning.condition*exp_block_id + (1 | id/exp_block_id), data = only_blocks(sub_walk_all[sub_walk_all$min_norm_distance < 10, ], 4:6))
tab_errors <- mixed_table_report(learning_slopes_errors)

tab <- merge(tab_distance[,c(3,4)], tab_errors[,c(3,4)], by = "names")
tab$names <- c("Intercept", "Block number", "Learning condition", "Block - Learning condition interaction")
colnames(tab) <- c("Predictor", "Significance for distance improvement", "Significance for error improvement")
kable(tab)
```


### Something about pointing?
```{r, echo=F, results='hide'}
err_phase <- lmer(abs_error ~ phase*learning.condition + (1 | id/phase), data = sub_sop_all)
car::Anova(err_phase, type = "III")

ggplot(sub_sop_all, aes(phase, abs_error, color = learning.condition)) +
  stat_summary(fun.data=mean_cl_normal,position=position_dodge(width=0.1), width = 0.5, geom="errorbar") + 
  stat_summary(fun.y=mean,position=position_identity(), size = 1.25, geom="line")

aov(abs_error ~ learning.condition, data = sub_sop_all)
```

## Summary
We can see different rates of learning speed across different conditions, with significant differences between groups in their performance in the post-switch block. Participants learning in the real world are performing the best after the switch, but only significanly different from the group learning on the desktop. All conditions show some level of transfer, demonstrated both by general improvement from the first block to the fourth, as well as decreased errors in desktop and treadmill VR group after the switch to the real environment. All groups get to the same level of performance after the three blocks post-switch moving in the real world, not differing in errors made nor in distance in the last block. But we also observed that whilst real world training leads to almost perfect performance in the pre-switch block, not differing from the last block in error rate and only slightly in distance. It takes another 3 blocks in the real world for the groups which learned on the treadmill or the desktop to achieve same performance, again pointing to qualitative difference between different modalities.

*I am unsure if it is worth adding the rest of the dataset in (the VR - VR and real - VR groups), but the performance of Treadmill VR groups in the distance traveled gets exactly to the same level as the real world performance in the 6th block (so the VR - VR group performs the same after the 6 blocks as VR - real and real - VR). Which is nice comparison to the results we get here about the distance performance in the real world being almost perfect in the 3rd block and the rest of the groups slowly "getting there".*

*The VR groups still demonstrate larger error difference in the 4-6th block, making more errors overall than the real world groups (about 2 errors per trial more). I believe it can be explained by either social difference - participants not wanting to "ask as much" in the real world, or accidental walking into a door due to treadmill control issues. Also, particiapnts told me that the VR is more "inviting" to test all the doors that are being passed (just in case), because it is more tiring to get back to one in case of a mistake. Anyway, not sure it it warranties adding more groups to the study, but at least the data are consistent and distance performance gets to the same level even in the VR*

We found difference in relative improvement rate 
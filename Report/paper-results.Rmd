---
title: "R Notebook"
output: github_document
---

```{r setup, include=FALSE}
#needs lme and car in the top because of the recode
library(car)
library(lme4)
library(lmerTest)
library(ggplot2)
library(ez)
library(Hmisc)
library(dplyr)
library(reshape2)
library(broom)
library(knitr)
library(papaja)
library(googlesheets)

source("../Scripts/paper-prepare.R")
source("helpers/block-t-test-improvement.R")
source("helpers/block-t-test.R")
source("helpers/block-measure.R")

settings <- load_google_sheets()
df_participants <- settings$participants
df_participants <- df_participants %>% mutate(condition=paste0(First.phase,"-",Second.phase))
df_participants <- df_participants %>% filter(condition %in% c("vr-real", "real-real", "ve-real"))
overview <- settings$versions
overview <- overview %>% mutate(condition=paste0(First.phase,"-",Second.phase))
overview <- overview %>% filter(condition %in% c("vr-real", "real-real", "ve-real"))

#green, violet, blue, yellow, pink, dark blue
estimote_pal <- c('#b6d6c1', '#54003d','#93d5f6','#d5d215','#f3aacb','#2d2556')
theme_set(theme_minimal())

### Try running it without outliers
sub_walk_all[!is.na(sub_walk_all$norm_distance) & sub_walk_all$norm_distance > 3, c("distance", "min_norm_distance")] <- NA

condition_all_sums <- sub_walk_all %>% 
  group_by(exp_block_id, id, learning.condition) %>% 
  summarise(sum.distance = sum(min_norm_distance, na.rm = T), 
            sum.errors = sum(errors, na.rm = T),
            mean.distance = mean(min_norm_distance, na.rm = T),
            mean.error = mean(errors, na.rm = T))


df_errors <- block_measure(condition_all_sums, "sum.errors")
df_distance <- block_measure(condition_all_sums, "mean.distance")

df_wide_errors <- dcast(condition_all_sums, learning.condition + id ~ exp_block_id, value.var = "sum.errors")
df_wide_distance <- dcast(condition_all_sums, learning.condition + id ~ exp_block_id, value.var = "mean.distance")
colnames(df_wide_errors) <- c("learning.condition","id","block1", "block2","block3","block4","block5","block6")
colnames(df_wide_distance) <- c("learning.condition","id","block1", "block2","block3","block4","block5","block6")
df_wide_errors$block34 <- (df_wide_errors$block3-df_wide_errors$block4)/(df_wide_errors$block3+df_wide_errors$block4)
df_wide_distance$block34 <- (df_wide_distance$block3-df_wide_distance$block4)/(df_wide_distance$block3+df_wide_distance$block4)

knitr::opts_chunk$set(echo=F, warning = F, message = F)

```

# Methods

```{r}
#plot for walking demonstration - need to load 
#load(file = "multi_smoothed.data")
#tw102 <- ls$tw102
#save(tw102, file = "tw102.data"
#load(file='tw102.data')
#plot_walk_trial(tw102$phase2,12)
#plot_walk_trial(tw102$phase1,12)
``
## Measures
We calculated participants’ walked distance in the building, time spent in each task, number of incorrectly visited doors during walking trials and pointing precision during pointing trials. The conditions were not perfectly comparable in measured distance and time, with the VR trials taking a little bit longer in both time and distance than the real world condition. We therefore min-normalised the distance traveled and the time for each task and environment by dividing the measure by any participant's best performance for that trial (therefore the result of 2 in min-normalised distance would mean the participant traveled double the distance than was necessary). We calculated all statistics on these normalised measures.

There was no difference between conditions at the start of the experiment in normalised distance (`r apa_print(aov(min_norm_distance ~ learning.condition, sub_walk_all[sub_walk_all$exp_block_id == 1, ]))$statistic$learning_condition`), nor in number errors (`r apa_print(aov(errors ~ learning.condition, sub_walk_all[sub_walk_all$exp_block_id == 1, ]))$statistic$learning_condition`), but we observed a significant difference in normalised time (`r apa_print(aov(min_norm_time ~ learning.condition, sub_walk_all[sub_walk_all$exp_block_id == 1, ]))$statistic$learning_condition`), due to the treadmill VR participants taking longer times to get used to the device and therefore having longer trial times at the start. We removed distance or time measurements larger than 3 standard deviations from the mean (a total of `r round((nrow(sub_walk_all[!is.na(sub_walk_all$norm_distance) & sub_walk_all$norm_distance > 3, ])/nrow(sub_walk_all))* 100, 1)` percent of trials).

## Participants
```{r}
finished <- overview %>% group_by(condition) %>% count(finished)
finished <- dcast(finished, condition~finished)
finished$percent <- round(finished$yes/(finished$yes+finished$no), 2)
is_ok <- overview %>% filter(finished=="yes") %>% group_by(condition) %>% count(is_ok)
is_ok <- dcast(is_ok, condition~is_ok)
is_ok$percent <- round(is_ok$yes/(is_ok$yes+is_ok$no), 2)
```
A total of `r sum(overview$is_ok == "yes", na.rm=T) + sum(finished$no)` undergraduate students at UC Davis (M = `r round(mean(df_participants[df_participants$finished == "yes",]$age, na.rm=T),1)`, SD = `r round(sd(df_participants[df_participants$finished == "yes",]$age, na.rm=T), 1)`) particiapted in the study in exchange for a study credit. `r sum(finished$no)` participants didn't finish due to motion sickness and `r sum(is_ok$no)` were removed due to a technical failure of the real world tracking systen. Only `r finished[finished$condition == "vr-real","percent"] * 100` percent the students being able to finish Treadmill VR learning condition. Each students was randomly assigned a condition and a randomised set of goals before arrival. 

# Results

First we wanted to assess that there was some level of transfer in all conditions. All conditions demonstrate better performance in the post-switch block than in the first block (see table XXX) , suggesting some level of transfer happening in all learning conditions.
```{r}
df <- data.frame(condition=rep("",3),dist=rep("",3), err=rep("",3), stringsAsFactors = F)
df_dist <- block_t_test_improvement(sub_walk_all, 1, 4,"min_norm_distance")
df_error <- block_t_test_improvement(sub_walk_all, 1, 4,"errors")
for(i in 1:length(names(df_dist))){
  name <- names(df_dist)[i]
  df[i,"condition"] <- name
  df[i, "dist"] <- apa_print(df_dist[[name]])$full_result
  df[i, "err"] <- apa_print(df_error[[name]])$full_result
}
colnames(df) <- c("Learning condition", "Distance improvement", "Errors improvement")
df$`Learning condition` <- dplyr::recode(df$`Learning condition`, "real-real" = "Real", "ve-real" = "Desktop", "vr-real" = "Treadmill VR", "real-vr" = "Real", "vr-vr" = "Treadmill VR")
kable(df, caption="Paired T test comparing individual performance improvement from block 1-4 in different conditions")
```

Looking at the progression of distance and error rate performance over the course of the experiment, we can see that all conditions show incremental improvement which is only hindered by the switch in the desktop learning condition (see fig. XXX and fig. XXX), but in the Treadmill VR condition we see constant improvement unobstructed by the environment switch.
```{r}
ggplot(sub_walk_all, aes(x = exp_block_id, y = min_norm_distance, color = factor(learning.condition))) +
  stat_summary(fun.data=mean_cl_normal,position=position_dodge(width=0.1), width = 0.5, geom="errorbar") + 
  stat_summary(fun.y=mean,position=position_identity(), size = 1.25, geom="line") + 
  geom_vline(xintercept = 3.5, linetype=4) +
  ylab("Average normalised trial distance") + xlab("Block") + 
  scale_color_manual(values=estimote_pal,"Learning condition") + guides(fill=F)

ggplot(sub_walk_all, aes(x = exp_block_id, y = errors, color = factor(learning.condition))) +
  stat_summary(fun.data=mean_cl_normal,position=position_dodge(width=0.1), width = 0.5, geom="errorbar") + 
  stat_summary(fun.y=mean,position=position_identity(), size = 1.25, geom="line") + 
  geom_vline(xintercept = 3.5, linetype=4) +
  ylab("Number of mistakes in office decision") + xlab("Block number") + 
  scale_color_manual(values=estimote_pal, "Learning condition")
```

Participant's performance in the first pre-switch phase shows different rates of learning rate, demonstrated by an interaction between block bumber and learning condition for error rate performance (see table XX). 

**this is a mixed model with id/block id random term. Significance was measured with Anova from car package, but I read it is not a good way to measure significance and we shouldn't even use significance in mixed models. I'll check on past cognition papers to see how and if they do it.**

```{r}
learning_slopes_distance <- lmer(min_norm_distance ~ learning.condition*exp_block_id + (1 | id/exp_block_id), data = sub_walk_all[sub_walk_all$exp_block_id %in% 1:3,])
tab_distance <- mixed_table_report(learning_slopes_distance)

learning_slopes_errors <- lmer(errors ~ learning.condition*exp_block_id + (1 | id/exp_block_id), data = sub_walk_all[sub_walk_all$exp_block_id %in% 1:3,])
tab_errors <- mixed_table_report(learning_slopes_errors)

tab <- merge(tab_distance[,c(3,4)], tab_errors[,c(3,4)], by = "names")
tab$names <- c("Intercept", "Block number", "Learning condition", "Block - Learning condition interaction")
colnames(tab) <- c("Predictor", "Significance for distance improvement", "Significance for error improvement")
kable(tab)
#ggplot(sub_walk_all[sub_walk_all$exp_block_id %in% 1:3,], aes(exp_block_id, min_norm_distance, color=learning.condition)) + geom_point() + geom_smooth(method="lm", se=F)
#ggplot(sub_walk_all[sub_walk_all$exp_block_id %in% 1:3,], aes(exp_block_id, errors, color=learning.condition)) + geom_point() + geom_smooth(method="lm", se=F)

```


### Performance change after modality switch
```{r}
aov_point_phase1 <- aov(abs_error ~ learning.condition, data = sub_sop_all[sub_sop_all$phase == 1,])
```
We found significant difference between different modalities  in the pre-switch block in distance performance (`r apa_print(aov(min_norm_distance ~ learning.condition, only_blocks(sub_walk_all, 3)))$statistic`), and the error rate as well (`r  apa_print(aov(errors ~ learning.condition, only_blocks(sub_walk_all, 3)))$statistic`). We also obseved a difference between conditions in their pointing performance at the end of the first phase (`r apa_print(aov_point_phase1)$statistic$learning_condition`), with real group performing significantly better than the Treadmill VR group (`r tukey_report(TukeyHSD(aov_point_phase1), condition = "Treadmill VR-Real", delta_name = "angle error")`) and the Desktop group (`r tukey_report(TukeyHSD(aov_point_phase1), condition = "Real-Desktop", delta_name = "angle error")`).

Mixed effect models with individual random effect show significant effect of the of the learning condition on rate of improvement from pre-switch to post-switch block (See table XXX).

```{r}
learning_slopes_distance <- lmer(min_norm_distance ~ learning.condition*exp_block_id + (1 | id/exp_block_id), data = only_blocks(sub_walk_all, 3:4))
tab_distance <- mixed_table_report(learning_slopes_distance)

learning_slopes_errors <- lmer(errors ~ learning.condition*exp_block_id + (1 | id/exp_block_id), data = only_blocks(sub_walk_all, 3:4))
tab_errors <- mixed_table_report(learning_slopes_errors)

tab <- merge(tab_distance[,c(3,4)], tab_errors[,c(3,4)], by = "names")
tab$names <- c("Intercept", "Block number", "Learning condition", "Block - Learning condition interaction")
colnames(tab) <- c("Predictor", "Significance for distance improvement", "Significance for error improvement")
kable(tab)
```

Running separate pairwise t-tests to see individual performance change from pre-switch to post-switch block, we see significant improvement in errors made across all conditions, and marginally significant worsening in distance performance in the desktop condition.
```{r, echo=F}
df <- data.frame(condition=rep("",3),dist=rep("",3), err=rep("",3), stringsAsFactors = F)
df_dist <- block_t_test_improvement(sub_walk_all, 3, 4,"min_norm_distance")
df_error <- block_t_test_improvement(sub_walk_all, 3, 4,"errors")
for(i in 1:length(names(df_dist))){
  name <- names(df_dist)[i]
  df[i,"condition"] <- name
  df[i, "dist"] <- apa_print(df_dist[[name]])$full_result
  df[i, "err"] <- apa_print(df_error[[name]])$full_result
}
colnames(df) <- c("Learning condition", "Distance improvement", "Errors improvement")
df$`Learning condition` <- dplyr::recode(df$`Learning condition`, "real-real" = "Real", "ve-real" = "Desktop", "vr-real" = "Treadmill VR", "real-vr" = "Real", "vr-vr" = "Treadmill VR")
kable(df, caption="Paired T test comparing individual performance improvement from pre switch block to post-switch block in different conditions")
```

We can see that all participants improved in the number of errors made, but neigher group is significantly worse or better in the post-switch block in the path travelled (although participants who learned on the desktop performed slightly worse, the significance is only marginal).

### Relative performance change after switch
To assess the level of performance change form directly before and after the switch, we calculated personal improvement score as (block3-block4)/(block3+block4)´. This allowed us to directly compare the relative improvement or deterioration from pre-switch block to the post-switch one.

```{r, echo=F}
ggplot(df_distance, aes(x = learning.condition, y = block34.diff, fill = learning.condition)) + 
  geom_bar(stat = 'identity', position="dodge") + 
  geom_errorbar(aes(ymin=block34.diff-block34.se, ymax=block34.diff+block34.se), width=.2, position=position_dodge(.9)) +
  scale_fill_manual(values = estimote_pal) +
  ylab("Mean normalised distance improvement from block 3 to 4") + xlab("Learning condition")  + guides(fill=F)

ggplot(df_errors, aes(x = learning.condition, y = block34.diff, fill = learning.condition)) + 
  geom_bar(stat = 'identity', position="dodge") + 
  geom_errorbar(aes(ymin=block34.diff-block34.se, ymax=block34.diff+block34.se), width=.2, position=position_dodge(.9)) +
  scale_fill_manual(values = estimote_pal) +
  ylab("Mean sum of errors improvement after switch")+ xlab("Learning condition") + guides(fill=F)
```

Comparing the performance change in different conditions using ANOVA, we see marginally significant difference between groups in the distance improvement (`r apa_print(aov(block34 ~ learning.condition, df_wide_distance))$statistic$learning_condition`), but  there are significant differences in error rate improvement (`r apa_print(aov(block34 ~ learning.condition, df_wide_errors))$statistic$learning_condition`). This is consistent with the mixed model result which pointed at an interaction between block number and learning condition.

Tukey post-hoc tests show significant difference between error rate improvement between the group that learned in the real world and that which learned on the desktop.
```{r, echo=F}
kable(tukey_report_table(TukeyHSD(x=aov(block34 ~ learning.condition, df_wide_errors), conf.level=0.95), "learning.condition"))
```

### Post Switch performance

In the post-switch block, we can still see a significant difference between groups in their distance performance (`r apa_print(aov(min_norm_distance ~ learning.condition, sub_walk_all[sub_walk_all$exp_block_id == 4,]))$statistic$learning_condition`) and error rate as well (`r apa_print(aov(errors ~ learning.condition, sub_walk_all[sub_walk_all$exp_block_id == 4,]))$statistic$learning_condition`), with groups learning on the treadmill or desktop performing worse than those learning in the real world. This between group difference is still present in the 2nd block of the second phase (block 5) in error rate `r  apa_print(aov(errors ~ learning.condition, only_blocks(sub_walk_all, 5)))$statistic` and marginally in distance `r apa_print(aov(min_norm_distance ~ learning.condition, only_blocks(sub_walk_all, 5)))$statistic`. In the last testing block (block 6), we found no differences among the groups in either error rate (`r apa_print(aov(errors ~ learning.condition, only_blocks(sub_walk_all, 6)))$statistic`), nor pointing performance (`r apa_print(aov(abs_error ~ learning.condition, data = sub_sop_all[sub_sop_all$phase == 2,]))$statistic$learning_condition`) but we still see small difference in distance performance (`r apa_print(aov(min_norm_distance ~ learning.condition, only_blocks(sub_walk_all, 6)))$statistic`).

### Improvement from the pre-switch block to the final block
Assuming the performance platoes around the 6the block, looking at participants performance in the pre-switch block and the last block we can deduce what margin of improvement is still possible. We see that participants learning in the real world no longer improve in error rate after the 3rd block `r apa_print(block_t_test_improvement(sub_walk_all, 4, 6, "errors")[["real-real"]])$statistic`, but they are continuously improving in distance performance `r apa_print(block_t_test_improvement(sub_walk_all, 4, 6, "min_norm_distance")[["real-real"]])$statistic`. 

*Not sure about the mixed models here. I think they are OK, but I haven't published anything with them before*

Looking at the linear mixed model prediction for the distance and error improvement in the second phase (post-switch), we can see that both learning condition and interaction with the experiment progression is significant, showing that there are different learning rates for different learning conditions even after the switch to the real world. 

```{r}
learning_slopes_distance <- lmer(min_norm_distance ~ learning.condition*exp_block_id + (1 | id/exp_block_id), data = only_blocks(sub_walk_all[sub_walk_all$min_norm_distance < 10, ], 4:6))
tab_distance <- mixed_table_report(learning_slopes_distance)

learning_slopes_errors <- lmer(errors ~ learning.condition*exp_block_id + (1 | id/exp_block_id), data = only_blocks(sub_walk_all[sub_walk_all$min_norm_distance < 10, ], 4:6))
tab_errors <- mixed_table_report(learning_slopes_errors)

tab <- merge(tab_distance[,c(3,4)], tab_errors[,c(3,4)], by = "names")
tab$names <- c("Intercept", "Block number", "Learning condition", "Block - Learning condition interaction")
colnames(tab) <- c("Predictor", "Significance for distance improvement", "Significance for error improvement")
kable(tab)
```

```{r, echo=F, results='hide'}
err_phase <- lmer(abs_error ~ phase*learning.condition + (1 | id/phase), data = sub_sop_all)
car::Anova(err_phase, type = "III")

ggplot(sub_sop_all, aes(phase, abs_error, color = learning.condition)) +
  stat_summary(fun.data=mean_cl_normal,position=position_dodge(width=0.1), width = 0.5, geom="errorbar") + 
  stat_summary(fun.y=mean,position=position_identity(), size = 1.25, geom="line")

aov(abs_error ~ learning.condition, data = sub_sop_all)
```

## Summary
We can see different rates of learning speed across different conditions, with significant differences between groups in their performance in the post-switch block. Participants learning in the real world are performing the best after the switch, but only significanly different from the group learning on the desktop. All conditions show some level of transfer, demonstrated both by general improvement from the first block to the fourth, as well as decreased errors in desktop and treadmill VR group after the switch to the real environment. All groups get to the same level of performance after the three blocks post-switch moving in the real world, not differing in errors made nor in distance in the last block. But we also observed that whilst real world training leads to almost perfect performance in the pre-switch block, not differing from the last block in error rate and only slightly in distance. It takes another 3 blocks in the real world for the groups which learned on the treadmill or the desktop to achieve same performance, again pointing to qualitative difference between different modalities.

*I am unsure if it is worth adding the rest of the dataset in (the VR - VR and real - VR groups), but the performance of Treadmill VR groups in the distance traveled gets exactly to the same level as the real world performance in the 6th block (so the VR - VR group performs the same after the 6 blocks as VR - real and real - VR). Which is nice comparison to the results we get here about the distance performance in the real world being almost perfect in the 3rd block and the rest of the groups slowly "getting there".*

*The VR groups still demonstrate larger error difference in the 4-6th block, making more errors overall than the real world groups (about 2 errors per trial more). I believe it can be explained by either social difference - participants not wanting to "ask as much" in the real world, or accidental walking into a door due to treadmill control issues. Also, particiapnts told me that the VR is more "inviting" to test all the doors that are being passed (just in case), because it is more tiring to get back to one in case of a mistake. Anyway, not sure it it warranties adding more groups to the study, but at least the data are consistent and distance performance gets to the same level even in the VR*

If we assume no transfer to happen, we should observe the same performance in the real world testing in the second phase to be similar to the real world learning group's performance in the first phase. In other terms, the switch groups should demonstrate same performance in the post-switch block as the real world group in the first exposition. But we clearly observe that the post-switch performance is better than the first block performance across all conditions.

